import { MetadataRoute } from "next";

/**
 * Robots.txt
 *
 * PURPOSE:
 * - Define what search engines can crawl
 * - Protect private/authenticated areas
 * - Expose public marketing & SEO pages
 *
 * FLOW (SERVER / CRAWLER):
 * crawler request â†’ robots generation â†’ rules returned â†’ sitemap linked
 */

export default function robots(): MetadataRoute.Robots {
  console.log("ğŸ¤– [robots] robots.txt requested by crawler");
  console.log("ğŸ“Œ [robots] Preparing crawl rules for public visibility");
  console.log("ğŸ”’ [robots] Blocking private/authenticated areas");

  const rules: MetadataRoute.Robots["rules"] = [
    {
      userAgent: "*",

      // âœ… Public, SEO-valuable pages
      allow: ["/", "/discover", "/about"],

      // ğŸ”’ Private / user-specific / sensitive
      disallow: [
        "/auth",
        "/chat",
        "/profile",
        "/admin",
        "/payment",
      ],
    },
  ];

  console.log("ğŸ§­ [robots] Crawl rules prepared:", rules);
  console.log("ğŸ—ºï¸ [robots] Sitemap reference attached");

  return {
    rules,
    sitemap: "https://instantconnect.com/sitemap.xml", // update when live
  };
}
